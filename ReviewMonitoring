# coding:utf-8
import pandas as pd
import numpy as np
import jieba
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB 
from sklearn.pipeline import make_pipeline 
from sklearn import metrics
import matplotlib.pyplot as plt
import csv


'''
文本处理 数据分割
origin_df = pd.read_csv("train.csv")
#print(df.iloc[:,:])#原本用ix。但在pandas的1.0.0版本开始，移除了DataFrame.ix 方法。故用iloc代替
origin_df['label'],origin_df['comment'] = origin_df['label\tcomment'].str.split('\t').str
keep_col = ['label','comment']
df = origin_df[keep_col]
df.to_csv('czz.csv',index=False)#将分割后的数据文本写入czz表中，并把序号列去掉
df = pd.read_csv('czz.csv')
df.head()
#print(df)
'''

df = pd.read_csv('czz.csv')
#print(df.iloc[:,1::])   #输出训练集数据
print("训练集数据大小",df.shape)
df['sentiment'] = df['label'].apply(lambda x:1 if x>0 else 0)
'''
df=df.drop_duplicates() ## 去掉重复的评论，我们将数据复制为原有数据的三倍
df=df.dropna()
print("去重后的数据大小",df.shape)
'''

#分词
X1 = pd.concat([df[['comment']],df[['comment']],df[['comment']]])
y1 = pd.concat([df.sentiment,df.sentiment,df.sentiment])
X1.columns = ['comment']
X1.reset_index #重置索引或其级别。重置数据帧索引，并使用默认索引代替。如果
print('翻倍后训练集评论数据大小',X1.shape)

def chinese_word_cut(mytext):
    return " ".join(jieba.cut(mytext))
#对于评论进行中文字符切分
X1['cut_comment'] = X1["comment"].apply(chinese_word_cut)
X1['cut_comment'].head()

#对于训练集自身进行分割 测试
#导入sklearn中的数据分割模块，设定test数据集大小，shuffle默认Ture
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X1,y1,random_state=42,test_size=0.3)
#print("y_train:",y_train)
#print("y_test",y_test)

print(type(X_train))

#获取停用词
def get_custom_stopwords(stop_words_file):
    with open(stop_words_file,encoding="utf-8") as f:
        custom_stopwords_list=[i.strip() for i in f.readlines()]
    return custom_stopwords_list
stop_words_file = 'stopwords.txt'
stopwords = get_custom_stopwords(stop_words_file)#获取停用词
#print("停用词",stopwords)
#停用词 ['aboard', 'about', 'about', 'about', 'above'···

#导入词袋模型
#from sklearn.feature_extraction.text import CountVectorizer
vect = CountVectorizer()  #实例化
vect#查看参数

#将分割后的文本进行fit_transform
#vect.fit_transform(X_trian["cut_comment"])
#vect.fit_transform(X_train["cut_comment"]).toarray().shape
#提取特征
pd.DataFrame(vect.fit_transform(X_train["cut_comment"]).toarray(),columns=vect.get_feature_names()).iloc[:,0:25].head()
original_feature_names = vect.get_feature_names()
print("特征名", original_feature_names)
print("初始特征名数量",len(original_feature_names))
vect = CountVectorizer(token_pattern=u'(?u)\\b[^\\d\\W]\\w+\\b',stop_words=frozenset(stopwords)) # 去除停用词，匹配以数字开头的非单词字符
pd.DataFrame(vect.fit_transform(X_train['cut_comment']).toarray(), columns=vect.get_feature_names()).head()
new_feature_names = vect.get_feature_names()
print("去除停用词后特征名", new_feature_names)
print("去除停用词后特征名数量",len(new_feature_names))

#模型构建
#多项式朴素贝叶斯：MultinomialNB主要用于离散特征分类，例如文本分类单词统计，以出现的次数作为特征值
nb = MultinomialNB()
pipe = make_pipeline(vect, nb)
pipe.steps #查看pipeline的步骤（与pipeline相似）

print("\n-----------------从训练集中分出训练用的测试集X_test和训练集X_train-----------------------")
print("-----------------对分出测试集X_test和训练集X_train进行训练预测评判-----------------------")
#训练模型
pipe.fit(X_train.cut_comment, y_train)#根据训练集分割出来的评价数据 和 训练集的结果 训练模型

#训练集中分割出来的测试集 预测结果
y_pred = pipe.predict(X_test.cut_comment)#对测试集进行预测（其中包含了转化以及预测）
#print("对于分割出来的测试集的预测结果：",y_pred)

#模型对于测试集的准确率
Ptrain=metrics.accuracy_score(y_test,y_pred)
print("测试集的准确率：",Ptrain)

# 模型对于测试集的混淆矩阵
CM_train=metrics.confusion_matrix(y_test,y_pred)
print("\n模型对于测试集的混淆矩阵：\n",CM_train)

#plt画图找不到字体，需要手动设置：
plt.rcParams['font.sans-serif']=['SimHei'] #显示中文标签
plt.rcParams['axes.unicode_minus']=False

def get_confusion_matrix(conf,clas):
    fig,ax=plt.subplots(figsize=(2.5,2.5))
    ax.matshow(conf,cmap=plt.cm.Blues,alpha=0.3)
    tick_marks = np.arange(len(clas))
    plt.xticks(tick_marks,clas, rotation=45)
    plt.yticks(tick_marks, clas)
    for i in range(conf.shape[0]):
        for j in range(conf.shape[1]):
            ax.text(x=i,y=j,s=conf[i,j],
                   va='center',
                   ha='center')
    plt.xlabel("预测标签")
    plt.ylabel("真实标签")
conf=metrics.confusion_matrix(y_test,y_pred)
class_names=np.array(['0','1'])
get_confusion_matrix(np.array(conf),clas=class_names)
plt.show()

#对整个数据集进行预测分类
y_pred_all = pipe.predict(X1['cut_comment'])

#对整个样本集的预测正确率，
All_accuracy_score=metrics.accuracy_score(y1,y_pred_all)
print("\n整个样本集的预测正确率：",All_accuracy_score)
if All_accuracy_score > Ptrain:
    print("---整个数据集的准确率高于分割出来的测试集，有些过拟合---")
else:
    print("---整个数据集的准确率小于分割出来的测试集，没有过拟合---")

#初始样本中 正类与负类的数量
print("\n初始样本中正类与负类的数量：\n",y1.value_counts())

#整个数据集的混淆矩阵
CM_all_train = metrics.confusion_matrix(y1,y_pred_all)
print("\n整个数据集的混淆矩阵：\n",CM_all_train)

#检出率（正类总样本检出的比例  真正/（假阴+真正）   ）
All_recall_score=metrics.recall_score(y1,y_pred_all)
print("\n检出率:",All_recall_score)

#准确率 （检测出来的正类中真正类的比例  真正/（假阳+真正））
All_precision_score=metrics.precision_score(y1, y_pred_all)
print("准确率:",All_recall_score)

#F1值  （精确率和召回率的调和平均数  2*精准率*检出率/（精准率+检出率） ）
All_f1_score = metrics.f1_score(y_true=y1,y_pred=y_pred_all)
print("F1分数:",All_f1_score)

print("\n分类报告:\n",metrics.classification_report(y1, y_pred_all)) # 分类报告

print("------------------------对真实测试集X=real_test进行预测-------------------------------")

csv_test_data = pd.read_csv('test_new.csv')
df2 = pd.DataFrame(csv_test_data)
#print(df.iloc[:,1::])   #输出测试集数据
print("测试集数据大小",df2.shape)

X_real = df2[['comment']]
X_real.columns = ['comment']
print('训练集评论数据大小',X_real.shape)
#对于评论进行中文字符切分
X_real['cut_comment'] = X_real["comment"].apply(chinese_word_cut)
X_real['cut_comment'].head()
vect = CountVectorizer(token_pattern=u'(?u)\\b[^\\d\\W]\\w+\\b',stop_words=frozenset(stopwords)) # 去除停用词，匹配以数字开头的非单词字符
pd.DataFrame(vect.fit_transform(X_real['cut_comment']).toarray(), columns=vect.get_feature_names()).head()
real_feature_names = vect.get_feature_names()
print("测试集去除停用词后特征名", real_feature_names)
print("测试集去除停用词后特征名数量",len(real_feature_names))

#真正测试集 预测结果
real_pred = pipe.predict(X_real.cut_comment)
print("对于真实测试集的预测结果数量：",len(real_pred))
df2_pred = pd.DataFrame(real_pred,columns= ['label'])
#print(df2_pred)
df2_pred.to_csv('test_pred.csv',index=False)


